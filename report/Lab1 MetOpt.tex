\documentclass[a4paper, 14pt]{article}


\usepackage{cmap}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{color}
\usepackage{pgfplots}
\pgfplotsset{compat=1.9}



\begin{document}

	\renewcommand{\chaptername}{Лабораторная работа}
	\def\contentsname{Содержание}

	\begin{titlepage}
		\begin{center}
			\textsc{<<НАЦИОНАЛЬНЫЙ ИССЛЕДВАТЕЛЬСКИЙ УНИВЕРСИТЕТ ИТМО">\\[5mm]
			Факультет информационных технологий и программирования\\[2mm]
			Кафедра компьютерных технологий}

			\vfill

			\textbf{ОТЧЁТ ПО ЛАБОРАТОРНОЙ РАБОТЕ №1\\[3mm]
			Методы одномерной оптимизации\\[6mm]
			Вариант 4
			\\[20mm]
			}
		\end{center}

		\hfill
		\begin{minipage}{.5\textwidth}
			Выполнили студенты:\\[2mm]
			Ефимов Сергей Алексеевич\\
			группа: М3237\\[2mm]
			Соколов Александр Андреевич\\
			группа: М3234\\[5mm]

			Проверил:\\[2mm]
			доцент кафедры суетологии\\
			Пупкина Залупкина
		\end{minipage}%
		\vfill
		\begin{center}
			г. Санкт-Петербург
		\end{center}
	\end{titlepage}


	\section*{Постановка задачи}
	Задача лаборатрной работы  -- научиться реализовывать алгоритмы одномерной минимизации функции каждым из следуших способов:
	\begin{enumerate}
		\item Метод дихотомии
		\item Метод золотого сечения
		\item Метод Фиббоначи
		\item Метод парабол
		\item Комбинированный метод Брента
	\end{enumerate}
	Также необходимо решить задачу аналитически и привести отчет по результатам сравнения проведенных вычислений
	\section*{Ход работы}
	Исследуемая функция \[f(x) = x -   \ln(x)\] Интервал исследования [0.5, 4]
	\subsection*{ Аналитическое решение}
	Для наглядности приведем график функции на заданном интервале:

	\begin{tikzpicture}
		\begin{axis}[
		title = x - ln(x),
		xlabel = {$x$},
		ylabel = {$y$},
		minor tick num = 2,
		xmin = 0.5,
		xmax = 4
		]
		\addplot[red] {x - ln(x)};
		\end{axis}
	\end{tikzpicture}

	Чтобы найти $/min$ функции необходимо найти ее критические точки(точки, в которых производная функции равна нуля $f'(x) = 0$):
	\[f'(x) = (x - \ln(x))' = x' - \ln(x)' = 0\]
	\[1 - \frac{1}{x} = 0 \Rightarrow x = 1\]
	Итак, мы нашли критическую точку, теперь осталось сравнить три значения функции -- в критической точке и в точках, являющиеся концами исследуемного отрезка. Пусть $x$ критическая точка, $a, b$ -начало и конец отрезка соответственно:
	\[f(a) = 0.5 - \ln(0.5) \approx 0.5 + 0.6931  \approx 1.1931\]
	\[f(b) = 4 - \ln(4) \approx 4 - 1.3863 \approx 2.6137\]
	\[f(x) = 1 - \ln(1) = 1\]
	Из приведенных выше вычислений можно утверждать, что $\min$ функции $f(x) = x - \ln(x)$ равен 1, при $x =1$, (точка $M(1,1)$)
	\subsection*{Метод дихотомии}
	Давайте рассмотрим на примере нашей функции метод дихотомии, который основывается на методе приближений, сокращая интревал поиска таким образом. что сохраняется инвариант - с каждой интерацией отрезок сокращается, минимум находится в пределах отрезка.

	Написав код к решению задачи данным способом(который можно увидеть ниже), мы имеем возможность привести таблицу выполнения данного алгоритма:\\\\
	\begin{tabular}{|c |c| c|}
		\hline
		Текущий интервал & Соотношение отрезков & текущий минимум \\
		\hline
		[0.5, 4] & - & - \\
		\hline
		[0.5, 2.25] & 1.999 & 1.375 \\
		\hline
		[0.5 1.375] & 1.999 & 0.9375 \\
		\hline
		[0.973, 1.375] & 1.999 & 1.156 \\
		\hline
		[0.973, 1.156] & 1.999 & 1.0468 \\
		\hline
		[0.973, 1.0468] & 1.999 & 0.992 \\
		\hline
		[0.992, 1.0468] & 1.999 & 1.019 \\
		\hline
		[0.992, 1.019] & 1.999 & 1.005 \\
		\hline
		[0.992, 1.005] & 1.999 & 0.999 \\
		\hline
		[0.999, 1.005] & 1.999 & 1.002 \\
		\hline
		[0.999, 1.002] & 1.999 & 1.001 \\
		\hline
		[0.999, 1.001] & 1.999 & 1.000 \\
		\hline
	\end{tabular} \\

	Исходя из данных таблицы можно утверждать, что каждую итерацию алгоритм сужает область поиска $\approx$ в 2 раза, что говорит о линейной сходимости данного алгоритма. Из этих соображений вытекает формула
	\[|x_n - x_{min}| \leqslant \frac{1}{2^n} \cdot (b - a)\]

	Чтобы оценить метод дихотомии давайте построим график зависимости кол-ва вычислений функции от логарифма точности:


	\begin{tikzpicture}
		\begin{axis}[
		title = График зависимости кол-ва вычислений функции от логарифма точности,
		xlabel = {$\ln(\epsilon)$},
		ylabel = {Кол-во вычислений}
		]
		\addplot coordinates {
			( -6.907, 11 )
			( -9.21, 15 )
			( -11.513, 18 )
			( -13.816, 21 )
			( -16.118, 25 )
			( -18.42, 28 )};
		\end{axis}
	\end{tikzpicture}

	Исходя из графика можно говорить о том, что кол-во вычислений данного метода линейно зависит от $\ln(\epsilon)$ причем чем больше $\ln(\epsilon)$, тем меньше кол-во вычислений, иными словами алгоритм делает больше итераций, если задаваемая точность выше, что логично\\
	
	Итого: 
	\begin{itemize}
		\item Преимущества
			\begin{itemize}
				\item Метод является одним из простых в реализации
				\item Обеспечивает грантированную сходимость
			\end{itemize}
		\item Недостатки
			\begin{itemize}
			\item Сходимость метода всегда постоянна т.е. Метод никогда не сойдется быстрее чем в наихудшем случае
			\end{itemize}
	\end{itemize}
	
	\subsection*{Метод золотого сечения}
	Теперь рассмотрим метод Золотого сечения. Принцип, который и дал название методу говорит о том, что мы будем сокращать интервал поиска нашего решения, причем делить текущий отрезок в прпорциях золотого сечения.
	
	Давайте посмотри на точки, которые мы получим на очередном этапе выполнения данного метода и проанализируем полученные данные:\\
	
	\begin{tabular}{|c |c| c| c| c|}
		\hline
		Интервал & Соотношение отрезков & текущий минимум & x1 & x2\\
		\hline
		[0.5, 4] & - & -& - & - \\
		\hline
		[0.5, 2.663] & 1.618 & 1.582 & 1.326 & 1.837\\
\hline
[0.5, 1.837] & 1.618 & 1.168 & 1.011 & 1.326\\
\hline
[0.5, 1.326] & 1.618 & 0.913 & 0.816 & 1.011\\
\hline
[0.816, 1.326] & 1.618 & 1.071 & 1.011 & 1.131\\
\hline
[0.816, 1.131] & 1.618 & 0.973 & 0.936 & 1.011\\
\hline
[0.936, 1.131] & 1.618 & 1.034 & 1.011 & 1.057\\
\hline
[0.936, 1.057] & 1.618 & 0.996 & 0.982 & 1.011\\
\hline
[0.982, 1.057] & 1.618 & 1.019 & 1.011 & 1.028\\
\hline
[0.982, 1.028] & 1.618 & 1.005 & 1 & 1.011\\
\hline
[0.982, 1.011] & 1.618 & 0.996 & 0.993 & 1\\
\hline
[0.993, 1.011] & 1.618 & 1.002 & 1 & 1.004\\
\hline
[0.993, 1.004] & 1.618 & 0.998 & 0.997 & 1\\
\hline
[0.997, 1.004] & 1.618 & 1.001 & 1 & 1.001\\
\hline
[0.997, 1.001] & 1.618 & 0.999 & 0.999 & 1\\
\hline
[0.999, 1.001] & 1.618 & 1 & 1 & 1\\
\hline
[0.999, 1] & 1.618 & 1 & 0.999 & 1\\
\hline
[0.999, 1] & 1.618 & 1 & 1 & 1\\
\hline
[1, 1] & 1.618 & 1 & 1 & 1\\
\hline
	\end{tabular} \\\\
	Посмотрев на таблицу можно говорить о следущих вещах:
		\begin{enumerate}
		\item Соотношение отрезков всегда одинаковое(пропорция золотого сечения) что говорит о его гарантированной сходимости и скорости сходимости порядка $n$(линейная сходимость)
		\item Различия с методом дихотомии
		\end{enumerate}
		
		Чтобы сделать некоторые выводы по методу, давайте приведем график зависимости кол-ва итераций от $\ln(\epsilon)$
		
		\begin{tikzpicture}
		\begin{axis}[
		title = График зависимости кол-ва вычислений функции от логарифма точности,
		xlabel = {$\ln(\epsilon)$},
		ylabel = {Кол-во вычислений}
		]
		\addplot coordinates {
			(-9.21034, 20)
			(-11.5129, 25)
			(-13.8155, 29)
			(-16.1181, 34)
			(-18.4207, 39)
			(-20.7233, 44)};
		\end{axis}
	\end{tikzpicture}
	На графике явно видна линейная зависимость. Но чем же тогда отличается метод дихотомии от метода Золотого сечения? Стоит обратить внимание на эффективность данного алгоритма засчет деления отрезка в пропорциях золотого сечения. \\
	 Итого: 
	\begin{itemize}
		\item Преимущества
			\begin{itemize}
				\item Метод обладает неплохой эффективностью, выше чем у метода дихотомии, но существуют методы с более высокой эффективностью
			\end{itemize}
		\item Недостатки
			\begin{itemize}
			\item Метод выолняет большое кол-во итераций, поэтому велика возможность накопления ошибки, что не есть хорошо
			\end{itemize}
	\end{itemize}
	
	\subsection*{Метод Фиббоначи}
	Очередной метод, который мы рассмотрим - метод Фиббоначи. Проанализированные ранее методы схожи с данным способом оптимизации. Его идея опять же заключается в последовательном делении отрезка поиска, НО есть различия, данный метод делит текущий отрезок на подотрезки следущим образом, что сохраняется равенство:
	
	
	На отрезке $[a_k, b_k]$ имеем: 
	\[
	\lambda_k = a_k + \frac{F_{n - k - 1}}{F_{n - k + 1}} \cdot (b_k - a_k)\]\[
	\mu_k = a_k + \frac{F_{n - k}}{F_{n - k + 1}} \cdot (b_k - a_k)
	\]
	
 Где $k = 1, 2, 3, 4, ..., n - 1 $ и $n$ - Общее число вычислений, а $\lambda_k, \mu_k$ - вычисляемые точки на итерации метода. 
 
Чтобы говорить о дальнейшем анализе метода, давайте построим таблицу значений: \\

\begin{tabular}{|c |c| c| c| c|}
		\hline
		Интервал & Соотношение отрезков & текущий минимум & $\lambda_k$ & $\mu_k$\\
		\hline
		[0.5, 4] & - & -& - & - \\
		\hline
[0.5, 2.663] & 1.618 & 1.582 & 1.326 & 1.837\\
\hline
[0.5, 1.837] & 1.618 & 1.168 & 1.011 & 1.326\\
\hline
[0.5, 1.326] & 1.618 & 0.913 & 0.816 & 1.011\\
\hline
[0.816, 1.326] & 1.618 & 1.071 & 1.011 & 1.131\\
\hline
[0.816, 1.131] & 1.618 & 0.973 & 0.936 & 1.011\\
\hline
[0.936, 1.131] & 1.618 & 1.034 & 1.011 & 1.057\\
\hline
[0.936, 1.057] & 1.618 & 0.996 & 0.982 & 1.011\\
\hline
[0.982, 1.057] & 1.618 & 1.019 & 1.011 & 1.028\\
\hline
[0.982, 1.028] & 1.618 & 1.005 & 1 & 1.011\\
\hline
[0.982, 1.011] & 1.618 & 0.996 & 0.993 & 1\\
\hline
[0.993, 1.011] & 1.618 & 1.002 & 1 & 1.004\\
\hline
[0.993, 1.004] & 1.618 & 0.998 & 0.997 & 1\\
\hline
[0.997, 1.004] & 1.618 & 1.001 & 1 & 1.001\\
\hline
[0.997, 1.001] & 1.618 & 0.999 & 0.999 & 1\\
\hline
[0.999, 1.001] & 1.618 & 1 & 1 & 1\\
\hline
[0.999, 1] & 1.618 & 1 & 0.999 & 1\\
\hline
[0.999, 1] & 1.618 & 1 & 1 & 1\\
\hline
[1, 1] & 1.618 & 1 & 1 & 1\\
\hline
	\end{tabular} \\\\

 Можно заметить, что таблица занчений очень похожа на таблицу значений метода Зологото сечения. Все различие с предыдущим подходом заключается в том, что коэфицент сокращения интервала поиска не статичен, он может меняться от итерации к итерации 
  
  Для большего кол-ва данных приведем график заыисимости от логарифма вычислений
  
  \begin{tikzpicture}
		\begin{axis}[
		title = График зависимости кол-ва вычислений функции от логарифма точности,
		xlabel = {$\ln(\epsilon)$},
		ylabel = {Кол-во вычислений}
		]
		\addplot coordinates {
			(-9.21034, 20)
			(-11.5129, 25)
			(-13.8155, 29)
			(-16.1181, 34)
			(-18.4207, 39)
			(-20.7233, 44)};
		\end{axis}
	\end{tikzpicture}
	
	График получился аналогичный графику метода Золотого сечения. 
	Можно подвести промежуточный итог:
	
	\begin{itemize}
		\item Преимущества
			\begin{itemize}
				\item Метод является <<надстройкой>> метода золотого сечения, которая увеличивает эффективность алгоритма
 			\end{itemize}
		\item Недостатки
			\begin{itemize}
			\item Необходимо знать $n$ - кол-во итераций которое мы будем совершать 
			\end{itemize}
	\end{itemize}
	
	\subsection*{Метод парабол}
	Метод парабол принципиально другой метод оптимизации, нежели мы рассматривали ранее. Его основная идея заключается в нахождении квадратичной функции, с помощью которой мы попытаемся аппроксимировать минимизируемую функцию
	\[p(x) = ax^2 + bx + c\]
	Пусть имеются три точки x1 < x2 < x3 такие, что интервал [x1, x3] содержит точку минимума функции f. Тогда
коэффициенты аппроксимирующей параболы a, b, c могут быть найдены путем решения системы линейных
уравнений:
\[ax^{2}_i + bx_i + c_i = f(x_i), i = 1, 2, 3 ....\]

Минимум такой параболы равен:
\[u = -\frac{a}{2b} = x_2 - \frac{(x_2 - x_1)^2(f_2 - f_3) - (x_2 - x_3)^2(f_2 - f_3)}{2[(x_2 - x_1)(f_2 - f_3) - (x_2 - x_3)(f_2 - f_3)]}\]

Этот мимимум и бует минимумом исходной функции на данной итерации
 
 Построим таблицу значений данного метода:\\
 
 \begin{tabular}{|c |c| c|}
		\hline
		Интервал & Соотношение отрезков & минимум параболы \\
		\hline
		[0.5, 4] & - & - \\
		\hline
		[0.5, 2.25] & 2 & 1.375\\
\hline
[0.5, 1.218] & 2.437 & 0.859\\
\hline
[0.912, 1.218] & 2.343 & 1.065\\
\hline
[0.912, 1.027] & 2.66 & 0.969\\
\hline
[0.912, 1.005] & 1.23 & 0.958\\
\hline
[0.912, 1.001] & 1.05 & 0.956\\
\hline
[0.912, 1] & 1.008 & 0.956\\
\hline
[0.912, 1] & 1.002 & 0.956\\
\hline
[1, 1] & $\infty$ & 1\\
\hline
	\end{tabular} \\\\
	
	Стоит заметить, что данный метод совершил намного меньше итераций чем предыдущие. 
	
	Чтобы убедиться в этом построим график зависимости кол-ва вычилений от логарифма задаваемой точности:
	
	   \begin{tikzpicture}
		\begin{axis}[
		title = График зависимости кол-ва вычислений функции от логарифма точности,
		xlabel = {$\ln(\epsilon)$},
		ylabel = {Кол-во вычислений}
		]
		\addplot coordinates {
			(-0.693147, 2)
(-0.798508, 2)
(-0.916291, 2)
(-1.04982, 2.3)
(-1.20397, 2.5)
(-1.38629, 2.7)
(-1.60944, 3)
(-1.89712, 3.2)
(-2.30259, 4)
(-2.99573, 5)
(-2.99573, 5)
(-3.10109, 6)
(-3.21888, 13)
(-3.35241, 13)
(-3.50656, 13)
(-3.68888, 13)
(-3.91202, 13)
(-4.19971, 13)
(-4.60517, 13)
(-5.29832, 13)};
		\end{axis}
	\end{tikzpicture}
	
	Грфик говорит о квадратичной сходимости, которая дает свои плюсы и минусы
	
	\begin{itemize}
		\item Преимущества
			\begin{itemize}
				\item Если необходима высокая точность решения, то данный метод хорошо подходит
				\item Скорость сходимости. Алгоритм имеет квадратичную скорость сходимости
 			\end{itemize}
		\item Недостатки
			\begin{itemize}
			\item Метод не надежен, Существуют функции на которых алгоритм дает сбой.
			\end{itemize}
	\end{itemize}
	
	
	\subsection*{Комбинированный метод Брента}
	Данный метод является комбинированием 2 методов: метода золотого сечения и метода парабол. Метод парабол работает быстрее в малой окрестности решения, но может работать долго и неустойчиво на наччальных итерациях. Тут на помощь приходит метод золотого сечения. Более формально метод отслеживает занчение в 6 точках: 
	концы отрезка$(a, c), x, v, w, u $, где $x$текущий минимум, $w$ - второе снизу значению функции, $v$ - предыдущее занчение $w$. Точка $u$ - корень аппроксимирующей параболы, которая будет использоваться если:
	  
	  \begin{itemize}
		\item $u \in [a, c]$
		\item $u$ отстоит от точки $x$ не более, чем на половину от длины предпредыдущего шага
			
	\end{itemize}
	
	Приведем таблицу значений нового метода: \\
	
	\begin{tabular}{|c |c| c| c| c| c|}
		\hline
		Интервал & Соотношение отрезков & текущий минимум & $x$ & $w$ & $v$\\
		\hline
		[0.5, 4] & - & -& - & -& - \\
		\hline
[0.5, 2.663] & 1.618 & 1.582 & 1.326 & 2.663 & 2.663\\
\hline
[0.5, 2.152] & 1.309 & 1.326 & 1.326 & 2.152 & 2.663\\
\hline
[0.5, 2.152] & 1 & 1.326 & 1.326 & 2.152 & 2.152\\
\hline
[0.5, 1.837] & 1.236 & 1.168 & 1.326 & 1.837 & 2.152\\
\hline
[0.5, 1.837] & 1 & 1.168 & 1.326 & 1.837 & 1.837\\
\hline
[0.5, 1.326] & 1.618 & 0.913 & 0.816 & 1.326 & 1.837\\
\hline
[0.816, 1.326] & 1.618 & 1.071 & 0.993 & 0.816 & 1.326\\
\hline
[0.816, 1.02] & 2.496 & 0.918 & 0.993 & 1.02 & 0.816\\
\hline
[0.993, 1.02] & 7.55 & 1.007 & 1.001 & 0.993 & 1.02\\
\hline
[0.993, 1.001] & 3.479 & 0.997 & 1 & 1.001 & 0.993\\
\hline
[0.996, 1.001] & 1.519 & 0.998 & 1 & 1.001 & 0.993\\
\hline
[0.997, 1.001] & 1.472 & 0.999 & 1 & 1.001 & 0.993\\
\hline
[0.998, 1.001] & 1.411 & 1 & 1 & 0.998 & 1.001\\
\hline
[0.999, 1.001] & 1.341 & 1 & 1 & 0.999 & 0.998\\
\hline
[0.999, 1.001] & 1.267 & 1 & 1 & 0.999 & 0.999\\
\hline
[0.999, 1.001] & 1.277 & 1 & 1 & 1.001 & 0.999\\
\hline
[1, 1.001] & 1.267 & 1 & 1 & 1 & 1.001\\
\hline
[1, 1] & 1.277 & 1 & 1 & 1 & 1\\
\hline
	\end{tabular} \\\\
	
	По колонке <<Соотношение отрезков>> можно отследить, что алгоритм чередует метод золотого сечения (1.618) и метод парабол. Давате взглянем на график зависимости кол-ва вычислений от лографима задаваемой точности: 
	
		   \begin{tikzpicture}
		\begin{axis}[
		title = График зависимости кол-ва вычислений функции от логарифма точности,
		xlabel = {$\ln(\epsilon)$},
		ylabel = {Кол-во вычислений}
		]
		\addplot coordinates {
			(-2.30259, 9)
(-4.60517, 10)
(-6.90776, 14)
(-9.21034, 22)
(-11.5129, 29)
(-13.8155, 34)
(-14.5087, 36)};
		\end{axis}
	\end{tikzpicture}
	
	Поведение графика говорит о линейной сходимости данного алгоритма, так же можно утвержать, что метод сойдется гарантировано за конечное число итераций.\\
	Итого: 
	
	\begin{itemize}
		\item Преимущества
			\begin{itemize}
				\item Обладает отличным контролем точности вычислений 
				\item Алгоритм гарантированно сходится за n итераций 
 			\end{itemize}
		\item Недостатки
			\begin{itemize}
			\item Скорость сходимости линейная
			\end{itemize}
	\end{itemize}
	
	\subsection*{Проверка алгоритмов на унимодальность}
	Давайте рассмотрим функцию:
	
	\begin{equation*}
		f(x) =
		\begin{cases}
			x < 1, f(x) = |x|\\
			x \eqslantgtr 1, f(x) = |x - 4| - 2
		\end{cases}
	\end{equation*}

	Ее график:
	\\

	\begin{tikzpicture}
		\begin{axis}[
			title = График приведенной нами многомодальной функции:,
			xlabel = x,
			ylabel = y
		]
			\addplot coordinates {
				( -3, 3 )
				( -2, 2 )
				( -1, 1 )
				( 0, 0 )
				( 1, 1 )
				( 2, 0 )
				( 3, -1 )
				( 4, -2 )
				( 5, -1 )
				( 6, 0 )
				( 7, 1 )};
		\end{axis}
	\end{tikzpicture}
  
  По графику несложно заметить, что на отрезке $[-3, 7]$ функция является многомодальной. Запустив каждый из алгоритмов на данной функции, увидим, что результаты отличаются друг от друга, причем ни один из них не является верным. Почему же так происходит? Пусть дали изначальный отрезок на котором находится 2 локальных минимума, мы знаем, что для каждого вышеизложенного алгоритма выполянется инвариант: <<алгоритм находит минимум функции, при условии что она унимодальна>>. Из данных рассуждений следует, что при очередной итерации алгоритм может шагнуть таким образом, что в области поиска окажется один локальный минимум, который не является глобальным, но т.к. алгоритм этого не поймет, для него это будет обычным нахождением минимума на отрезке.
  
  
  \section*{Вывод}
  Подводя итог всему вышсказанному, можно утверждать, что каждый из методов имеет свои преимущества и недостатки. 
\end{document}
